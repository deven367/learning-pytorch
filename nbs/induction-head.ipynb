{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b61453-eea4-4217-a011-41256810b3d6",
   "metadata": {},
   "source": [
    "# induction head\n",
    "> This nb contains some exploration for the induction head\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fa52f-f031-43c5-97da-6f32424f043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c198e2c-054a-448f-8289-d72d250cb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InductionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(InductionHead, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)  # Query, Key, Value projections\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)  # Output projection\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # Project input to Q, K, V\n",
    "        qkv = self.qkv_proj(x)  # Shape: (seq_len, batch_size, 3 * embed_dim)\n",
    "        qkv = qkv.reshape(seq_len, batch_size, self.num_heads, 3 * embed_dim // self.num_heads)\n",
    "        qkv = qkv.permute(2, 1, 0, 3)  # (num_heads, batch_size, seq_len, 3 * embed_dim // num_heads)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # Split into Q, K, V\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.permute(1, 0, 2).contiguous().reshape(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb136e-0d68-44f1-bf63-277de3fed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InductionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(InductionHead, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)  # Query, Key, Value projections\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)  # Output projection\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, embed_dim)\n",
    "        seq_len, batch_size, embed_dim = x.size()\n",
    "\n",
    "        # Project input to Q, K, V\n",
    "        qkv = self.qkv_proj(x)  # Shape: (seq_len, batch_size, 3 * embed_dim)\n",
    "        qkv = qkv.reshape(seq_len, batch_size, 3, self.num_heads, embed_dim // self.num_heads)\n",
    "        qkv = qkv.permute(2, 3, 1, 0, 4)  # (3, num_heads, batch_size, seq_len, embed_dim_per_head)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]  # Split into Q, K, V\n",
    "\n",
    "        # Reshape Q, K, V for attention mechanism\n",
    "        # Q = Q.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\n",
    "        # K = K.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\n",
    "        # V = V.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\n",
    "\n",
    "        Q = Q.reshape(batch_size * self.num_heads, seq_len, embed_dim )\n",
    "        K = K.reshape(batch_size * self.num_heads, seq_len, embed_dim )\n",
    "        V = V.reshape(batch_size * self.num_heads, seq_len, embed_dim )\n",
    "\n",
    "        # Apply attention (query, key, value)\n",
    "        attn_output, attn_weights = self.attention(Q, K, V)\n",
    "\n",
    "        # Reshape attention output back\n",
    "        attn_output = attn_output.reshape(batch_size, self.num_heads, seq_len, embed_dim // self.num_heads)\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).reshape(seq_len, batch_size, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94061bca-bcd4-4516-9b42-54b2ae9cab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "induction_head = InductionHead(embed_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57032d-fc09-45af-904b-3b55859dc4a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[160, 10, 512]' is invalid for input of size 102400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dummy input (batch_size, seq_len, embed_dim)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, embed_dim)\n\u001b[0;32m----> 3\u001b[0m output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43minduction_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected: (batch_size, seq_len, embed_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mInductionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Split into Q, K, V\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Reshape Q, K, V for attention mechanism\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Q = Q.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# K = K.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# V = V.reshape(batch_size * self.num_heads, seq_len, embed_dim // self.num_heads)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, seq_len, embed_dim )\n\u001b[1;32m     27\u001b[0m V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, seq_len, embed_dim )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[160, 10, 512]' is invalid for input of size 102400"
     ]
    }
   ],
   "source": [
    "# Dummy input (batch_size, seq_len, embed_dim)\n",
    "x = torch.randn(10, 20, embed_dim)\n",
    "output, attn_weights = induction_head(x)\n",
    "print(output.shape)  # Expected: (batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970c40d-022f-4fda-8a86-c2301a1e7317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
